% !TEX root = ./../main.tex
\chapter{MD software, parallelization and hardware acceleration}
\label{chap:software}
%Manuale GROMACS 3.17.5 PP/PME ranks
%http://www.gromacs.org
In this chapter we present the main tools used in this thesis: the \href{http://www.gromacs.org}{\gromacs{}} 
package for \ac{MD} and the \href{http://www.plumed.org}{\plumed{}} tool for metadynamics. These techniques are 
nowadays implemented according to parallel schemes that allow to exploit all the computational resources of the 
modern multi--core CPUs and multi--node cluster. These parallelization scheme are fundamental to deal with large 
biological systems, and we shall describe here the main ones used in this thesis work. Eventually, we provide the 
main input parameters used to setup the \ac{MD} and metadynamics runs. 


\section{GROMACS package}
The \ac{MD} software and the main analysis tools used in this thesis work are part of the 
\href{http://www.gromacs.org}{\gromacs{}}\cite{gromacsManual} package. It is compatible with both atomistic and 
\ac{CG} \acp{FF} and supports a variety of options and parameters such as different thermostat and barostat 
algorithms, different methods to treat the electrostatic interaction such as the \ac{ESM} and the \ac{PME} 
method, different integrators, cut--off schemes and so on. \gromacs{} supports different kind of parallelization 
techniques, that is, it is compatible with both multi--core single processor (home PC) and multi--core 
multi--processor (cluster) architectures with the support of both 
\href{https://computing.llnl.gov/tutorials/mpi/}{MPI}
%\footnote{https://computing.llnl.gov/tutorials/mpi/} 
and \href{https://computing.llnl.gov/tutorials/openMP/}{OpenMP}
%\footnote{https://computing.llnl.gov/tutorials/openMP/}
libraries for the inter--nodes and inter--cores information exchange and the support of different hardware 
accelerated frameworks such as \href{https://developer.nvidia.com/cuda-zone}{NVIDIA CUDA}\textsuperscript{®}
%\footnote{https://developer.nvidia.com/cuda-zone}
, \href{https://www.khronos.org/opencl/}{OpenCL}™
%\footnote{https://www.khronos.org/opencl/}
 and \href{http://www.openacc.org}{OpenACC}.
 %\footnote{http://www.openacc.org}  

\subsection{Parallelization}
Parallelization refers to the possibility of running a program, in this case an \ac{MD} simulation, using more 
then one CPU or more then one core per CPU or both. The efficiency of a parallelization scheme is mainly 
determined by how much information each core and/or CPU has to exchange with the others, the least the better. 
The reason is that the libraries (MPI and OpenMP) needed for the exchange of the information are not as fast as 
the cores are. Hence when the waiting time for the information exchange becomes higher than the total time spent 
in executing the parallel code, the program has reached its scaling limit, that is, the use of more cores is no 
more efficient.

\ac{MD} simulations are parallelized according to one main parallelization scheme: the domain decomposition. It 
exploits the local character of most of the interactions used in common \acp{FF}. The simulation box is divided 
into cells, each of which is assigned to a core. The core then manages all the particles that lie in that cell, 
their mutual interactions and the interactions between them and their neighbors, that could reside in the 
neighbor cells (cores). In order to minimize the communication between different cores each cell has to minimize 
its surface--to--volume ratio. When using orthogonal simulations box, this requirement means that the cells have 
to be as cubic as possible. Moreover, if the cells are not too small, the particle update can be carried out 
every time the neighbor list is updated and not at every \ac{MD} step. A common choice is to use cells greater 
than two times the cut-off radius. \gromacs{} reaches its scaling limit when each cell contains about 
$100 \div 200$ particles.

\subsection{PP and PME nodes}
When using the \ac{PME} method for electrostatic interactions a special parallelization scheme is needed in order 
to speed up the simulation. The reason is related to the fact that the \ac{PME} method, as described in 
section~\ref{sec:electrostaticInt}, intrinsically need to know all the particle positions in the whole simulation 
box. In a domain decomposition scheme this results in a massive use of the so called \textit{all--to--all} 
communication, i.e. all cores have to communicate to each other. This leads to a loss of computational 
performance. In order to minimize the all--to--all communications, \gromacs{} implement the possibility to assign 
a subset of the cores exclusively to the calculation of the energy contribution of electrostatic interactions 
using the \ac{PME} method, while the rest of the work is assigned to the remaining cores, called the \ac{PP} 
cores. 
\begin{figure}[h!t]
	\centering
	\includegraphics[width=0.8\textwidth]{./img/PMENodes}
	\caption{Left: example of the use of $8$ cores for both \acs{PP} and \acs{PME}. Right: same but $2$ cores are used only for \acs{PME} and the other for the \acs{PP} contribution. Red arrows represent the all--to--all communications while the blu arrows are the communications between one \acs{PP} core and one \acs{PME} core. Taken from \cite{gromacsManual}.}%
	\label{fig:PMENodes}
\end{figure} 
In figure~(\ref{fig:PMENodes}) a schematic comparison of the all--to--all communications needed in the case of 
\ac{PP}/\ac{PME} in the same cores and that of separate \ac{PME} cores is shown. As one can see the 
all--to--all communications (red arrows) are smaller for the separate \acs{PME} scheme than those for 
\ac{PP}/\ac{PME} scheme. With this parallelization scheme the workflow of figure~(\ref{fig:PPCore}) is updated as 
shown in figure~(\ref{fig:PPPMECores}). 
\begin{figure}[h!t]
	\centering
	\includegraphics[width=0.8\textwidth]{./img/Schemi/PPPMECores}
	\caption{Schematic workflow of the load balancing between \acs{PP} and \acs{PME} cores.}
	\label{fig:PPPMECores}
\end{figure}

\paragraph{\textbf{PME Domain decomposition}} To optimize the load in the \ac{PME} cores a domain decomposition 
is still used but with a $2$D or a ``pencil'' scheme. This is done because the \ac{FFT} method is very inefficient 
if parallelized with too many cores due to the all--to--all communications, thus the domain decomposition acts on 
the $xy$--plane while the $z$ axis is assigned to one core; pencil refers to the shape of the domain at high 
parallelization. Moreover the number of domains along the $x$ axis have to be equal to the number of domains in 
the \ac{PP} decomposition and eventually a $1$D scheme can be used along the $y$ axis only if the \ac{PP} 
decomposition has one domain along the $x$ axis. To avoid superfluous communication of coordinates and forces 
between the \ac{PP} and \ac{PME} cores, the number of cells along the $x$ axis should ideally be the same or a 
multiple of the number of the \ac{PME} cores. Most of this issues are taken into account automatically by 
\gromacs{} at the beginning of the simulation.

\paragraph{\textbf{PP -- PME load balancing}} In order to achieve the best performance a good balancing of the 
load between \ac{PP} and \ac{PME} cores is needed, i.e. no core should wait for any other. In \gromacs{} this is 
done balancing the real part and the \ac{PME} part of the electrostatic interaction. Since the real part is 
computed on \ac{PP} cores as non--bonded interaction, changing the Coulomb cut--off between the real and the 
\ac{PME} part will adjust the load balancing between \ac{PP} and \ac{PME} cores. Of course this has to be done 
keeping unchanged the desired accuracy of the electrostatic energy contribution. In \gromacs{} the ratio between 
the starting cut-off radius and the \ac{PME} grid spacing gives the accuracy of the electrostatic energy 
contribution. Then changing both the Coulomb cut--off and \ac{PME} grid spacing, leaving the ratio unchanged, 
will adjust the \ac{PP} -- \ac{PME} load without affecting the accuracy. We have to stress out that at the 
beginning of the simulation the Coulomb, the Van der Waals and the pair list cut--off radii are set equal. Then 
\gromacs{} only adjust the ratio between the Coulomb cut--off and the \ac{PME} grid spacing, without affecting the 
cut--off of the pair list or of the Van der Waals interaction.

\subsection{Hardware acceleration}
Excluding \ac{PME}, another very time consuming part of an \ac{MD} simulation is the computation of the 
non--bonded interactions. The way to speed up a simulation when more cores cannot be used (e.g. the system has 
reached its scaling limit or there are no more cores available) is to use hardware accelerator. 
By offloading the computation of the non--bonded interactions to the hardware accelerator, the CPU has the 
possibility to do \textit{in concurrency} other calculations such as the \ac{PME} calculations. \gromacs{} 
completely supports the NVIDIA GPU with the NVIDIA CUDA\textsuperscript{®} framework acceleration together with 
the use of the Verlet cut--off scheme. Other GPUs or other cut--off schemes are partially supported with OpenCL or 
OpenACC libraries. 

The speed up of the simulation by offloading the non--bonded interactions to a GPU using NVIDIA 
CUDA\textsuperscript{®} is of the order of $2-4$ times faster than that of a non accelerated simulation. In 
table~(\ref{tab:performance}) we report a comparison of the performance obtained with a \ac{CG} system composed by 
a total of $7664$ particles.
\begin{table}[ht]
	\centering
	\begin{tabular}{lccc}
		\toprule
		\, 					& GPU\&\acs{PME} balance 	& GPU		& no GPU	\\ \midrule
		Performance [ns/day]& $3466$					& $1388$	& $847$		\\ \bottomrule
	\end{tabular}
	\caption{Performance comparison obtained with a system composed of $128$ phospholipids and $2000$ \ac{PW} beads for a total of $7664$ particles. The used PC has a quad core CPU at $4.00$~GHz and the NVIDIA Titan X GPU card. \gromacs version $5.1.1$.}%
	\label{tab:performance}
\end{table}
Moreover, to increase the performance, avoiding the waiting time of the CPU for the GPU to finish the non--bonded 
calculations, the same load balancing between \ac{PP} and \ac{PME} part can be used. Adjusting both the Coulomb 
cut--off and \ac{PME} grid spacing, leaving the ratio unchanged, will adjust the load between the real and the 
reciprocal part of the electrostatic energy contribution. Since the real part is computed by the GPU as 
non--bonded interaction, it will adjust the load balancing between the GPU and the CPU. Using an hardware 
accelerator the schematic workflow of figure~(\ref{fig:PPCore}) is updated to the one shown in 
figure~(\ref{fig:GPU}).
\begin{figure}[h!t]
	\centering
	\includegraphics[width=0.8\textwidth]{./img/Schemi/GPU}
	\caption{Schematic workflow of the calculation of the interactions with the use of a GPU as hardware accelerator.}%
	\label{fig:GPU}
\end{figure}

Basically and briefly the main difference, as showed in figure~(\ref{fig:GPUvsCPU}), between a CPU and a GPU is 
the total number of 
\begin{figure}[h!t]
	\centering
	\includegraphics[width=0.6\textwidth]{./img/GPUvsCPU}
	\caption{Schematic comparison between a CPU and a GPU.}
	\label{fig:GPUvsCPU}
\end{figure}
\textit{computing modules} assigned to do vectorized calculations i.e. calculation that involve many vectors of 
different length. Nowadays the total number of hardware cores in a high level CPU is eight cores. Instead, a 
middle level GPU has a number of computing modules greater than $10^3$. In the GPU case, the computing 
module is essentially a \ac{SIMD} computing unit which is composed by several cores, each one equipped with 
several \acp{ALU} and \acp{FPU}. A \ac{SIMD} unit is a specific module for vector calculation: it gives several 
instruction sets to perform the same instruction on a multiple data stream i.e. multiple data arranged in a 
vector fashion. In figure~(\ref{fig:simd}) a schematic representation of a \ac{SIMD} that execute in parallel 
four sums of two numbers
\begin{figure}[h!t]
	\centering
	\includegraphics[width=0.5\textwidth]{./img//simd/simd.pdf}
	\caption{Block diagram of a \acs{SIMD} computation on a four element vectors.}
	\label{fig:simd}
\end{figure}
i.e. the sum of each cell of the two vectors, is shown. Essentially this is possible thinking of a $32$--bit 
register as vector of four $8$--bit or two $16$--bit numbers, then the same instruction is applied to each cell 
pair of the vectors. Even a modern CPU has the possibility of \ac{SIMD} computation but the goal of the GPU 
implementation is the extremely high number of modules arranged in an efficient pipeline with respect to CPUs. 
Hence the \textit{concurrency} calculations on streamed data are much faster if executed on a GPU than on a CPU. 
On the contrary the CPU has the ability to perform several tasks in parallel (or approximately in parallel).

\section{PLUMED package}
The tool used for setting up and analyze a metadynamics simulation is the \href{http://www.plumed.org}{\plumed{}} 
package. It is an open source project composed by a set of libraries used to analyze the data obtained in a 
metadynamics simulation (first of all, to obtain the \ac{FES} from the history--dependent bias potential) or 
other advanced sampling techniques, such as umbrella sampling. Moreover it works together with the most popular 
\ac{MD} tools, included \gromacs{}. In this sense it is a sort of plug--in that inserts the advanced sampling code 
and algorithms inside the main \ac{MD} loop of the used \ac{MD} tool.

For setting up a metadynamics simulation \plumed{} requires an input file in which we have to specify what are the 
\acp{CV} and the parameters to be used in the metadynamics run. Moreover there is the possibility to add more 
features such as, online analysis or more output files and options.

For what concerns the parallelization we have noted that it is badly parallelized between more than one node, 
that is, we observe good computational performance if we use the parallelization between cores inside the same 
CPU with the OpenMP library and the use of the GPU as hardware accelerator. Otherwise, for example with more than 
one node of a cluster, we obtain worse computational performance. Moreover, depending on the system and the 
parameters of the metadynamics, we observe a general decrease of computational performance compared to an 
un--biased \ac{MD} simulation.

\newpage
\section{Simulation parameters}
To set up an \ac{MD} run, \gromacs{} needs the following input files and information:
\begin{itemize}
	\item the initial configuration file in which the position and eventually the velocity of all the particles and the three box dimensions are saved;%
	\item the input parameters file in which, following the \ac{FF} recipe, all the \ac{MD} parameters are given;
	\item the topology file in which all the particles of a system are described.
\end{itemize}

In table~(\ref{tab:inputParam}) we report the most important \ac{MD} parameters used in this thesis work. In 
table~(\ref{tab:inputParam}) ``VbT'' means Verlet--buffer--tolerance; ``VdW'' means Van der Waals; $T$ and $p$ 
coupling are the used thermostat and barostat algorithms, respectively; $\tau_T$ and $\tau_p$ are the related time 
constants; $p$ type, is the pressure coupling type: semi--isotropic means two independent pressure couplings for 
the $xy$ axes and for the $z$ axis; $K$ is the related compressibility (in this case equal for both pressure 
couplings). In general when starting an \ac{MD} simulation, an equilibration run (Eq. in 
table~(\ref{tab:inputParam})) of few nanoseconds, which also generates random velocities from the 
Maxwell–Boltzmann distribution at fixed temperature, is needed for equilibrating the volume of the simulation box 
and the whole system. Then one has to switch to the production run (Run in table~(\ref{tab:inputParam})) in which 
the chosen ensemble is sampled correctly. 

For what concerns the metadynamics parameters, in accordance with \cite{ourPaper}, in 
table~(\ref{tab:metadynParam}) we report the main input parameters used to set up a metadynamics run.

\begin{table}[h!t]
	\centering\footnotesize
	\begin{tabular}{lcccc}
		\toprule
		Input parameter & Eq. \acs{PME} & Run \acs{PME} & Eq. \acs{PME}\&\acs{PW} & Run \acs{PME}\&\acs{PW} \\ \toprule
		Time step [fs]		& $10$ & $20$ & $5$ & $20$ \\ \midrule
		cut--off scheme		& Verlet & Verlet & Verlet & Verlet \\ \midrule
		VbT [J/(mol ps)]	& $5$ & $5$ & $5$ & $5$ \\ \midrule
		Coulomb Type		& \acs{PME}	& \acs{PME}	& \acs{PME} & \acs{PME} \\ \midrule
		$r$ Coulomb	[nm]	& $1.2$ & $1.2$ & $1.2$ & $1.2$ \\ \midrule
		$\varepsilon_r$		& $15$ & $15$ & $2.5$ & $2.5$ \\ \midrule
		\acs{PME} grid [nm]	& $0.12$ & $0.12$ & $0.12$ & $0.12$ \\ \midrule
	    \acs{PME} order		& $4$ & $4$ & $4$ & $4$ \\ \midrule
		VdW type			& cut--off & cut--off & cut--off & cut--off \\ \midrule
		$r$ VdW [nm]		& $1.2$ & $1.2$ & $1.2$ & $1.2$ \\ \midrule
		$T$ coupling		& $v$--rescale & $v$--rescale & $v$--rescale & $v$--rescale \\ \midrule
		$T$ [K]				& $310$ & $310$ & $310$ & $310$  \\ \midrule
		$\tau_T$ [ps]		& $2.0$ & $2.0$ & $2.0$ & $2.0$ \\ \midrule
		$p$ coupling		& Berendsen & Parrinello--Rahman & Berendsen & Parrinello--Rahman \\ \midrule
		$p$ type			& semiisotropic & semiisotropic & semiisotropic & semiisotropic \\ \midrule
		$p$ [bar]			& $1.0$ & $1.0$ & $1.0$ & $1.0$ \\ \midrule
		$\tau_p$ [ps]		& $4.0$ & $12.0$ & $4.0$ & $12.0$ \\ \midrule
		$K$ [bar$^{-1}$]	& $4.5\cdot 10^{-5}$ & $3.0\cdot 10^{-4}$ & $4.5\cdot 10^{-5}$ & $3.0\cdot 10^{-4}$ \\ \midrule
		$v$ random			& yes ($310$~K) & no & yes ($310$~K) & no \\ \bottomrule 
	\end{tabular}
	\caption{Summary of the main input parameters used in our \acs{MD} runs.}
	\label{tab:inputParam}
\end{table}

\begin{table}[ht]
	\centering
	\begin{tabular}{llll}
		\toprule
		Deposition pace ($\tau$) & Gaussian height ($w$)& Gaussian width ($\delta s$) & Grid spacing \\ \toprule
		$50000$ \ac{MD} steps$^a$ & $2.479$~kJ$^b$ & $0.06$~nm & $0.005$~nm \\ \bottomrule
	\end{tabular}
	\caption{Main metadynamics parameters used in this thesis work. $^a$ With a $20$~fs time step it means a deposition of a Gaussian every nanoseconds. $^b$ At thermostat temperature ($310$~K) it is approximately equal to $1~k_BT$.}%
	\label{tab:metadynParam}
\end{table}
